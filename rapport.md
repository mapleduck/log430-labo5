# LOG430 - Rapport du laboratoire 02
Ã‰TS - LOG430 - Architecture logicielle - Hiver 2026 - Groupe 1

Ã‰tudiant: Yanni Haddar
Nom github: mapleduck
repo github: https://github.com/mapleduck/log430-labo5 et https://github.com/mapleduck/log430-labo5-payment

## Questions

Note prÃ©alabe: Je n'ai malheureusement pas pu accÃ©der Ã  ma VM, les tests de charges ont donc Ã©tÃ© roulÃ©s localement sur ma machine, avec les specs suivantes:
- Ubuntu Desktop 22.04
- 13th Gen Intel Core i7-1365U
- 32GB LPDDR5 6400 MT/s
Je reglerai le problÃ¨me avec la VM pour le prochain labo.


> ğŸ’¡ Question 1 : Combien d'utilisateurs faut-il pour que le Store Manager commence Ã  Ã©chouer dans votre environnement de test ? Pour rÃ©pondre Ã  cette question, comparez la ligne Failures et la ligne Users dans les graphiques.

Vers 122 users:

![1](./docs/img_rapport/1.png)

> ğŸ’¡ Question 2 : Sur l'onglet Statistics, comparez la diffÃ©rence entre les requÃªtes et les Ã©checs pour tous les endpoints. Combien d'entre eux Ã©chouent plus de 50 % du temps ?

Toutes les requÃªtes Ã©chouent bien au dessus de 50%. 81% des requÃªtes overall Ã©chouent:

![2](./docs/img_rapport/2.png)

> ğŸ’¡ Question 3 : Affichez quelques exemples des messages d'erreur affichÃ©s dans l'onglet Failures. Ces messages indiquent une dÃ©faillance dans quelle(s) partie(s) du Store Manager ? Par exemple, est-ce que le problÃ¨me vient du service Python / MySQL / Redis / autre ?

La plupart des Ã©ches viennent de Flask qui est overloaded (uniquement les get), mais une bonne patie vient aussi du serveur SQL qui ne peux pas handle tout les write (car les posts sont beaucoup plus couteux que les gets).

![3](./docs/img_rapport/3.png)

> ğŸ’¡ Question 4 : Sur l'onglet Statistics, comparez les rÃ©sultats actuels avec les rÃ©sultats du test de charge prÃ©cÃ©dent. Est-ce que vous voyez quelques diffÃ©rences dans les mÃ©triques pour l'endpoint POST /orders ?

Oui. Pour commencer (pas visible dans le tableau), le systÃ¨me a handle beaucoup plus de requests, passant de 31 Ã  58 RPS. Les requÃªtes sont rÃ©pondues beaucoup, beaucoup plus rapidement (voir toute les stats au milieu du tableau). Le taux d'Ã©chec, lui, n'a pas bougÃ© vraiment, restant Ã  80%. Mais cela reste une amÃ©lioration nette.

![4](./docs/img_rapport/4.png)

> ğŸ’¡ Question 5 : Si nous avions plus d'articles dans notre base de donnÃ©es (par exemple, 1 million), ou simplement plus d'articles par commande en moyenne, le temps de rÃ©ponse de l'endpoint POST /orders augmenterait-il, diminuerait-il ou resterait-il identique ?

Le temps de rÃ©ponse resterait relativement identique. MÃªme avec 1 million de produits, la recherche d'articles par product_id reste trÃ¨s performante car elle utilise la clÃ© primaire de la table Product, qui est trÃ¨s efficace selon mes recherches. Et grÃ¢ce Ã  l'optimisation n+1 rajoutÃ©e, une requÃªte rÃ©cupÃ¨re tout les prix d'un coup.

> ğŸ’¡ Question 6 : Sur l'onglet Statistics, comparez les rÃ©sultats actuels avec les rÃ©sultats du test de charge prÃ©cÃ©dent. Est-ce que vous voyez quelques diffÃ©rences significatives dans les mÃ©triques pour les endpoints POST /orders, GET /orders/reports/highest-spenders et GET /orders/reports/best-sellers ? Dans quelle mesure la performance s'est-elle amÃ©liorÃ©e ou dÃ©tÃ©riorÃ©e (par exemple, en pourcentage)?

Ã‰norme amÃ©lioration pour les GET (0% failure rate) et temps de rÃ©ponse divisÃ© par 5. Mais pour les POSTS seulement, aucune amÃ©lioration notable.

![5](./docs/img_rapport/5.png)

> ğŸ’¡ Question 7 : La gÃ©nÃ©ration de rapports repose dÃ©sormais entiÃ¨rement sur des requÃªtes adressÃ©es Ã  Redis, ce qui rÃ©duit la charge pesant sur MySQL. Cependant, le point de terminaison POST /orders reste Ã  la traÃ®ne par rapport aux autres en termes de performances dans notre scÃ©nario de test. Alors, qu'est-ce qui limite les performances de l'endpoint POST /orders ?

La performance de POST /orders est limitÃ©e par les opÃ©rations d'Ã©criture MySQL (il n'est pas sur REDIS) nÃ©cessaires pour garantir la persistance des donnÃ©es et la gestion des stocks. Contrairement aux rapports qui lisent un cache pre-calculated dans Redis, chaque commande doit valider et enregistrer plusieurs entries dnas l BD, etant limitÃ© par les ressources disques et le serveur MySQL.

> ğŸ’¡ Question 8 : Sur l'onglet Statistics, comparez les rÃ©sultats actuels avec les rÃ©sultats du test de charge prÃ©cÃ©dent. Est-ce que vous voyez quelques diffÃ©rences significatives dans les mÃ©triques pour les endpoints POST /orders, GET /orders/reports/highest-spenders et GET /orders/reports/best-sellers ? Dans quelle mesure la performance s'est-elle amÃ©liorÃ©e ou dÃ©tÃ©riorÃ©e (par exemple, en pourcentage) ? La rÃ©ponse dÃ©pendra de votre environnement d'exÃ©cution (par exemple, vous obtiendrez de meilleures performances en exÃ©cutant 2 instances de Store Manager sur 2 machines virtuelles plutÃ´t que sur une seule).

Il y a une nette dÃ©gradation des performanes par rapport au test prÃ©cÃ©dent, sauf dans une mÃ©trique: la rapiditÃ© de rÃ©ponse des GET. Le taux d'Ã©chec, qui Ã©tait Ã  35%, est passÃ© Ã  59%. Les RPS sont passÃ©es de 64 Ã  53.

Ma thÃ©orie est que, en Ã©tant sur une seule machine, l'effet de balance est contre-productif, car le nombre de coeurs sont limitÃ©s et les requÃªtes se partagent toutes les mÃªmes ressources pour leur exÃ©cution. Il n'y a pas de rÃ©el load balancing car tout roule sur la mÃªme machine. Il y a juste un risque augmentÃ© de collisions.

Je n'ai aucun doute qu'en ayant deux (ou mÃªme un cluster) de machines sur lesquelles il est rÃ©ellement possible de faire du load balancing, mÃªme si ces machines Ã©taient significativement plus faibles que mon laptop, les rÃ©sultats seraient notablement meilleurs, car nginx est optimisÃ©e pour cela, par pour tout rouler sur une seule machine.

![6](./docs/img_rapport/6.png)

> ğŸ’¡ Question 9 : Dans le fichier nginx.conf, il existe un attribut qui configure l'Ã©quilibrage de charge. Quelle politique d'Ã©quilibrage de charge utilisons-nous actuellement ? Consultez la documentation officielle de Nginx si vous avez des questions.

`least_conn` dans Upstream est le paramÃ¨tre utilisÃ©. Selon la doc, cette politique distribue de maniÃ¨re intelligente en envoyant les requÃªtes au serveur qui a le moins de connexions en cours Ã  cet instant.